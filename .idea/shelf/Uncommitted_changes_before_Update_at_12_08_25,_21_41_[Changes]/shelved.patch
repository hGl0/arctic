Index: vortexclust/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pathlib import Path\n\n# === Project Root === #\nROOT_DIR = Path(__file__).resolve().parent.parent\n\n# === Directory Paths === #\nDATA_DIR = ROOT_DIR / \"data\"\nOUTPUT_DIR = ROOT_DIR / \"output\"\nDOC_DIR = ROOT_DIR / \"doc\"\n\n# Optional: subfolders\nRAW_DATA_DIR = DATA_DIR / \"raw\"\nPROCESSED_DATA_DIR = DATA_DIR / \"processed\"\nFIGURES_DIR = OUTPUT_DIR / \"figures\"\nANIMATIONS_DIR = OUTPUT_DIR / \"animations\"\n\n# Ensure folders exist (optional)\nfor d in [OUTPUT_DIR, FIGURES_DIR, ANIMATIONS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# === Global Constants === #\nDEFAULT_TIMEZONE = \"UTC\"\nDEFAULT_LATITUDE_BAND = (50, 90)  # Degrees North, e.g. Arctic region\nDEFAULT_PRESSURE_LEVELS = [10, 50, 100]  # hPa for stratosphere\n\n# === Plotting Defaults === #\nPLOT_STYLE = \"seaborn-whitegrid\"\nCOLOR_PALETTE = \"viridis\"\nDPI = 150\nFIGSIZE = (10, 10)\n\n# === Logging Setup === #\nimport logging\n\nLOG_LEVEL = logging.INFO\nLOG_FORMAT = \"[%(levelname)s] %(asctime)s - %(message)s\"\nlogging.basicConfig(level=LOG_LEVEL, format=LOG_FORMAT)\n\nlogger = logging.getLogger(\"vortexclust\")\n\n# === Environment Flags === #\nDEBUG_MODE = False\nVERBOSE = True\n\n# === Utility Functions === #\ndef set_debug_mode(enabled: bool = True):\n    global DEBUG_MODE\n    DEBUG_MODE = enabled\n    logger.setLevel(logging.DEBUG if enabled else LOG_LEVEL)\n    logger.debug(\"Debug mode enabled.\" if enabled else \"Debug mode disabled.\")\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/vortexclust/config.py b/vortexclust/config.py
--- a/vortexclust/config.py	(revision 66ec76aea3fd9febcbe4e5d401f791f9440a4b84)
+++ b/vortexclust/config.py	(date 1755027440049)
@@ -44,6 +44,14 @@
 
 # === Utility Functions === #
 def set_debug_mode(enabled: bool = True):
+    r"""
+    Toggle debug logging for the package.
+
+    :param enabled: Whether to enable debug mode, defaults to True.
+    :type enabled: bool, optional
+
+    :return: None
+    """
     global DEBUG_MODE
     DEBUG_MODE = enabled
     logger.setLevel(logging.DEBUG if enabled else LOG_LEVEL)
Index: vortexclust/visualization/map.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\nfrom typing import Union, Tuple, Optional\n\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport matplotlib.cm as cm\nimport matplotlib.colors as mcolors\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nfrom matplotlib import animation\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.colors import ListedColormap\n\nfrom vortexclust.analysis.aggregator import apply_aggregation\nfrom vortexclust.analysis.geometry import compute_ellipse\nfrom vortexclust.core.utils import norm_series_df, validate_columns\nfrom vortexclust.io.paths import check_path\n\n# To Do Doc string\ndef plot_split(ax, sample, filled, color='red'):\n    # plot mother vortex\n    x_final, y_final, _ = compute_ellipse(sample.area, sample.ar, sample.theta, sample.loncent, sample.latcent)\n    ax.plot(x_final, y_final, linestyle='-.', color=color, label='Mother vortex')\n\n    # plot daughter vortices\n    for n in [1, 2]:\n        if pd.notnull(sample.get(f'area{n}')):\n            x, y, _ = compute_ellipse(sample[f'area{n}'], sample[f'ar{n}'], sample[f'theta{n}'],\n                                      sample[f'loncent{n}'], sample[f'latcent{n}'])\n            plot_ellipse(ax, x, y, sample[f'loncent{n}'], sample[f'latcent{n}'], filled=filled,\n                         label=f\"Split vortex {n}\", color=color)\n\ndef create_animation(df: pd.DataFrame, time_col: str, filled: bool,\n                     savegif: Optional[str] = None,\n                     split_col: Optional[str] = 'form',\n                     split: Optional[int] = 1,\n                     figsize: Optional[Tuple[int]] = (10,10)) -> None:\n    r\"\"\"\n    Creates an animation of ellipses over time using a DataFrame of ellipse parameters.\n\n    :param df: DataFrame containing ellipse parameters for each time step\n    :type df: pd.DataFrame\n    :param time_col: Name of the column in df that contains time information\n    :type time_col: str\n    :param filled: Whether to fill the ellipses or just draw outlines\n    :type filled: bool\n    :param split_col:\n    :type split_col: str\n    :param split:\n    :type split: int\n    :param figsize:\n    :type figsize: Tuple[int]\n    :param savegif: Path to save the animation as a GIF file, if None the animation is displayed instead\n    :type savegif: str, optional\n\n    :return: None\n    \"\"\"\n\n    df = df.sort_values(by=time_col)\n\n    fig = plt.figure(figsize=figsize)\n\n    def update(frame):\n        fig.clf()  # clear the figure entirely\n\n        fig.suptitle(str(df.iloc[frame][time_col]))\n        ax = fig.add_subplot(1, 1, 1, projection=ccrs.NorthPolarStereo())\n        ax.set_extent([-180, 180, 40, 90], crs=ccrs.PlateCarree())\n\n        # Add coastlines and gridlines\n        ax.coastlines()\n\n        row = df.iloc[frame]\n        color = 'tab:blue'\n\n        if row.get(split_col) == split:\n            plot_split(ax, row, filled, color)\n        else:\n            x, y, _ = compute_ellipse(row.area, row.ar, row.theta, row.loncent, row.latcent)\n            plot_ellipse(ax, x, y, row.loncent, row.latcent, filled)\n\n        # Add gridlines: 8 longitude lines every 45°, latitude every 10°\n        gl = ax.gridlines(crs=ccrs.PlateCarree(),\n                          draw_labels=True,\n                          color='gray',\n                          linestyle='--',\n                          linewidth=1)\n\n        gl.xlocator = mticker.FixedLocator(np.arange(-180, 181, 45))\n\n\n    ani = animation.FuncAnimation(fig, update, frames=len(df), interval=1000, repeat=False, blit=False)\n\n    if savegif:\n        ani.save(savegif, dpi=300, writer=animation.PillowWriter(fps=1))\n    else:\n        plt.show()\n    plt.close()\n\n\ndef create_polar_ax(figsize=(10,10)) -> Tuple[plt.Figure, plt.Axes]:\n    r\"\"\"\n    Creates a matplotlib figure and axis with North Polar Stereographic projection.\n\n    The function sets up a figure with coastlines and appropriate extent for Arctic visualization.\n\n    :return: A tuple containing:\n        - The matplotlib figure object\n        - The axis object with North Polar Stereographic projection\n    :rtype: tuple(matplotlib.figure.Figure, matplotlib.axes.Axes)\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize, subplot_kw={'projection': ccrs.NorthPolarStereo()})\n    ax.add_feature(cfeature.NaturalEarthFeature('physical', 'coastline', '50m',\n                                                edgecolor='black', facecolor='none'), linewidth=0.5)\n    ax.set_extent([-180, 180, 40, 90], crs=ccrs.PlateCarree())\n    return fig, ax\n\n\ndef plot_ellipse(ax: plt.Axes, x: np.ndarray, y: np.ndarray, loncent: float, latcent: float,\n                 filled: bool = True,\n                 color: str = 'red',\n                 label: str = 'Vortex ellipse') -> None:\n    r\"\"\"\n    Plots an ellipse on a given axis, with options for filled or outline representation.\n\n    :param ax: The matplotlib axis on which to plot\n    :type ax: matplotlib.axes.Axes\n    :param x: X-coordinates of the ellipse points\n    :type x: array-like\n    :param y: Y-coordinates of the ellipse points\n    :type y: array-like\n    :param loncent: Longitude of the ellipse center\n    :type loncent: float\n    :param latcent: Latitude of the ellipse center\n    :type latcent: float\n    :param filled: Whether to fill the ellipse or just draw the outline, defaults to True\n    :type filled: bool, optional\n    :param color: Color of the ellipse, defaults to 'red'\n    :type color: str, optional\n    :param label: Label for the ellipse in the legend, defaults to 'Vortex ellipse'\n    :type label: str, optional\n\n    :return: None\n    \"\"\"\n    # Coordinate validation\n    if not isinstance(x, (np.ndarray, list)) or not isinstance(y, (np.ndarray, list)):\n        raise TypeError(\"x and y must be array-like (e.g., np.ndarray or list).\")\n\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    if x.shape != y.shape:\n        raise ValueError(f\"x and y must have the same shape. Got {x.shape} and {y.shape}.\")\n\n    if len(x) == 0:\n        raise ValueError(\"x and y are empty.\")\n\n    if not np.isfinite(x).all() or not np.isfinite(y).all():\n        raise ValueError(\"x and y must not contain NaN or infinite values.\")\n\n    if filled:\n        ax.fill(x, y, color=color, label=label)\n    else:\n        ax.plot(x, y, color=color, label=label)\n    ax.scatter(loncent, latcent, color=color, marker='x', transform=ccrs.PlateCarree(), label='Center')\n    ax.legend(loc='upper left')\n\n\n\ndef plot_polar_stereo(\n        df: Union[pd.DataFrame, pd.Series],\n        P: Union[int, float] = 10,\n        T: Union[int, float] = -50,\n        mode: str = 'single',  # modify mode on given arguments?\n        **kwargs) -> None:\n    r\"\"\"\n    Plots polar stereographic projections of ellipses representing polar vortices.\n\n    This function supports multiple visualization modes: single plot, aggregated plot, \n    animation, subplots, or overlay of multiple ellipses.\n\n    :param df: DataFrame containing ellipse parameters (area, latcent, loncent, theta, ar)\n    :type df: pd.DataFrame or pd.Series\n    :param P: Pressure level in hPa, defaults to 10\n    :type P: int or float, optional\n    :param T: Temperature in degrees Celsius, defaults to -50\n    :type T: int or float, optional\n    :param mode: Visualization mode, one of 'single', 'aggregate', 'animate', 'subplot', or 'overlay', defaults to 'single'\n    :type mode: str, optional\n    :param kwargs: Additional arguments:\n        - `agg_func` (str or callable, optional): Aggregation function for 'aggregate' mode\n        - `time_col` (str, optional): Column name containing time data, required for 'animate' mode\n        - `max_subplots` (int, optional): Maximum number of subplots for 'subplot' mode, defaults to 4\n        - `savefig` (str, optional): Path to save the figure, defaults to None\n        - `filled` (bool, optional): Whether to fill ellipses or just draw outlines, defaults to False\n        - `cmap` (str, optional): Colormap for ellipses, defaults to 'viridis'\n        - `max_brightness` (float, optional): Maximum brightness for colormap, defaults to 0.7\n\n    :raises KeyError: If 'time_col' is not provided for 'animate' mode\n    :raises ValueError: If required columns are missing from the DataFrame\n\n    :return: None\n    \"\"\"\n    agg_func = kwargs.get('agg_func', None)\n    time_col = kwargs.get('time_col', None)\n    max_subplots = kwargs.get('max_subplots', 4)\n    savefig = kwargs.get('savefig', None)\n    filled = kwargs.get('filled', False)\n    cmap = kwargs.get('cmap', 'viridis')\n    max_brightness = kwargs.get('max_brightness', 0.7)\n    split = kwargs.get('split', 1) # does it make sense?\n    split_col = kwargs.get('split_col', 'form')\n    figsize = kwargs.get('figsize', (10, 10))\n\n    # for future use\n    \"\"\"lon = np.linspace(0, 360, 361)\n    lat = np.linspace(90, 40, 71)  # From the North Pole to mid-latitudes\n    lon2d, lat2d = np.meshgrid(lon, lat)\n\n    # compute geopotential height\n    T = T + ZERO_DEG  # convert T to Kelvin\n    z = R * T / g * np.log(P0 / P)\n    gph = np.full_like(lon2d, z)\n    \"\"\"\n\n    # deal with differences between pd.Series and pd.DataFrame\n    df = norm_series_df(df)\n    validate_columns(df, ['area', 'latcent', 'loncent', 'theta', 'ar'])\n\n    if mode == \"aggregate\":\n        df = apply_aggregation(df, agg_func=agg_func)\n        plot_polar_stereo(df, savefig=savefig, figsize=figsize)\n\n    elif mode == \"animate\":\n        if time_col is None:\n            raise KeyError(\"time_col required for animation mode\")\n        validate_columns(df, [time_col])\n        # To Do: adjust for split event\n        create_animation(df, time_col, filled, savefig, figsize=figsize)\n\n    elif mode == \"subplot\":\n        count = min(len(df), max_subplots)\n        ncols = math.ceil(math.sqrt(count))\n        nrows = math.ceil(count / ncols)\n\n        # Sort and check time column\n        if time_col and time_col in df.columns:\n            if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n                df[time_col] = pd.to_datetime(df[time_col])\n            df = df.sort_values(by=time_col).reset_index(drop=True)\n        else:\n            df = df.reset_index(drop=True)\n            time_col = None\n\n        groups = [df.loc[idx] for idx in np.array_split(df.index, count)]\n        fig, axs = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5 * nrows),\n                                subplot_kw={'projection': ccrs.NorthPolarStereo()})\n        axs = axs.flatten()\n\n        for i, (ax, group) in enumerate(zip(axs, groups)):\n            ax.set_extent([-180, 180, 40, 90], crs=ccrs.PlateCarree())\n            ax.add_feature(cfeature.NaturalEarthFeature('physical', 'coastline', '50m',\n                                                        edgecolor='black', facecolor='none'), linewidth=0.5)\n\n            if len(group) > 10:\n                print(f\"Warning: Subplot {i + 1} has {len(group)} ellipses. This may be hard to distinguish.\")\n\n            # Color setup\n            if time_col:\n                colors = cm.viridis(np.linspace(0, 1, len(group)))\n                labels = group[time_col].dt.strftime(\"%Y-%m-%d\").tolist()\n                title = f\"{labels[0]} to {labels[-1]}\"\n            else:\n                colors = cm.tab10(np.linspace(0, 1, len(group)))\n                labels = [f\"PV {j + 1}\" for j in range(len(group))]\n                title = f\"Subplot {i + 1}\"\n\n            for j, (_, row) in enumerate(group.iterrows()):\n                if row.get(split_col) == split:\n                    plot_split(ax, row, filled, color=colors[j])\n                else:\n                    x, y, _ = compute_ellipse(row.area, row.ar, row.theta, row.loncent, row.latcent)\n                    plot_ellipse(ax, x, y, row.loncent, row.latcent, filled=filled,\n                                    color=colors[j], label=labels[j])\n\n            # Add gridlines: 8 longitude lines every 45°, latitude every 10°\n            gl = ax.gridlines(crs=ccrs.PlateCarree(),\n                              draw_labels=True,\n                              color='gray',\n                              linestyle='--',\n                              linewidth=0.8)\n\n            gl.xlocator = mticker.FixedLocator(np.arange(-180, 181, 45))\n            ax.set_title(title)\n            ax.legend(loc='lower left', fontsize='small')\n\n            # Hide unused subplots\n        for j in range(len(groups), len(axs)):\n            fig.delaxes(axs[j])\n\n        plt.tight_layout()\n        if savefig:\n            check_path(savefig)\n            plt.savefig(savefig)\n        plt.show()\n\n    elif mode == \"overlay\":\n        original_cmap = plt.colormaps.get_cmap(cmap)\n        trimmed_colors = original_cmap(np.linspace(0, max_brightness, 256))\n        muted_cmap = ListedColormap(trimmed_colors)\n\n        fig, ax = create_polar_ax()\n\n        # create color gradient if time column is available\n        if time_col is not None:\n            if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n                df[time_col] = pd.to_datetime(df[time_col])\n            df = df.sort_values(by=time_col).reset_index(drop=True)\n            c = muted_cmap(np.linspace(0, 1, len(df)))\n        else:\n            c = ['tab:blue'] * len(df)\n\n        # Plot each sample with appropriate color\n        for i, row in df.iterrows():\n            if row.get(split_col) == split:\n                plot_split(ax, row, filled, color=c[i])\n            else:\n                x, y, _ = compute_ellipse(row.area, row.ar, row.theta, row.loncent, row.latcent)\n                label = 'Vortex ellipse' if i == 0 else None\n\n                if filled:\n                    ax.fill(x, y, color=c[i], alpha=0.5, label=label) if filled else ax.plot(x, y, color=c[i],\n                                                                                              label=label)\n                else:\n                    ax.plot(x, y, color=c[i], label=label)\n\n                center_label = 'Center' if i == 0 else None\n                ax.scatter(row.loncent, row.latcent, color=c[i], marker='x',\n                       transform=ccrs.PlateCarree(), label=center_label, zorder=5)\n        if time_col:\n            norm = mcolors.Normalize(vmin=0, vmax=len(df) - 1)\n            sm = cm.ScalarMappable(cmap=muted_cmap, norm=norm)\n            sm.set_array([])\n            cbar = plt.colorbar(sm, ax=ax, orientation='horizontal')\n            cbar.set_label(f'Time progression ({df[time_col].iloc[0].date()} → {df[time_col].iloc[-1].date()})')\n\n        # Add gridlines: 8 longitude lines every 45°, latitude every 10°\n        gl = ax.gridlines(crs=ccrs.PlateCarree(),\n                          draw_labels=True,\n                          color='gray',\n                          linestyle='--',\n                          linewidth=0.8)\n\n        gl.xlocator = mticker.FixedLocator(np.arange(-180, 181, 45))\n\n        # One legend\n        handles, labels = ax.get_legend_handles_labels()\n        unique = dict(zip(labels, handles))\n        ax.legend(unique.values(), unique.keys(), loc='upper left')\n\n        fig.autofmt_xdate(rotation=45)\n        if savefig:\n            check_path(savefig)\n            plt.savefig(savefig)\n        plt.show()\n\n    else:\n        for _, sample in df.iterrows():\n            # extract parameters for ellipse computation\n            area = sample.area  # km², example area\n            latcent = sample.latcent  # Latitude of vortex center\n            loncent = sample.loncent  # Longitude of vortex center\n            theta = sample.theta  # Angle of the major axis in degrees\n            ar = sample.ar  # Aspect ratio (major / minor axis)\n\n            x_final, y_final, _ = compute_ellipse(area, ar, theta, loncent, latcent)\n\n            fig, ax = create_polar_ax(figsize=figsize)\n\n            if sample.get(split_col) == split:\n                plot_split(ax, sample, filled)\n            else:\n                plot_ellipse(ax, x_final, y_final, loncent, latcent)\n\n            # Add gridlines: 8 longitude lines every 45°, latitude every 10°\n            gl = ax.gridlines(crs=ccrs.PlateCarree(),\n                              draw_labels=True,\n                              color='gray',\n                              linestyle='--',\n                              linewidth=1)\n\n            gl.xlocator = mticker.FixedLocator(np .arange(-180, 181, 45))\n            # gl.ylocator = mticker.FixedLocator(np.arange(40, 91, 10))\n\n            # plot_ellipse(ax, x_final, y_final, loncent, latcent, filled=filled)\n            # Add a legend\n            ax.legend(loc='upper left')\n            fig.autofmt_xdate(rotation=45)\n            if savefig:\n                check_path(savefig)\n                plt.savefig(savefig)\n            plt.show()\n\n            # Plot data as filled contours, show changing gph as shades of grey\n            # include color bar if so\n            # contour = ax.contourf(lon2d, lat2d, gph, levels=20, transform=ccrs.PlateCarree(), cmap='Greys')\n\n            # # Add contour lines for gph or something else\n            # ax.contour(lon2d, lat2d, gph, levels=10, colors='black', linewidths=1, transform=ccrs.PlateCarree())\n\n            # Add a colorbar\n            # cbar = plt.colorbar(contour, ax=ax, orientation='horizontal', pad=0.05, shrink=0.8)\n            # cbar.set_label('Geopotential Height (gpm)')
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/vortexclust/visualization/map.py b/vortexclust/visualization/map.py
--- a/vortexclust/visualization/map.py	(revision 66ec76aea3fd9febcbe4e5d401f791f9440a4b84)
+++ b/vortexclust/visualization/map.py	(date 1755027591793)
@@ -18,8 +18,22 @@
 from vortexclust.core.utils import norm_series_df, validate_columns
 from vortexclust.io.paths import check_path
 
-# To Do Doc string
+
 def plot_split(ax, sample, filled, color='red'):
+    r"""
+    Plot a split vortex event with mother and daughter ellipses.
+
+    :param ax: Axis on which to draw the ellipses.
+    :type ax: matplotlib.axes.Axes
+    :param sample: Data containing ellipse parameters for the split event.
+    :type sample: pandas.Series or dict-like
+    :param filled: Whether to fill the ellipses.
+    :type filled: bool
+    :param color: Color used for the ellipses, defaults to 'red'.
+    :type color: str, optional
+
+    :return: None
+    """
     # plot mother vortex
     x_final, y_final, _ = compute_ellipse(sample.area, sample.ar, sample.theta, sample.loncent, sample.latcent)
     ax.plot(x_final, y_final, linestyle='-.', color=color, label='Mother vortex')
@@ -412,4 +426,5 @@
 
             # Add a colorbar
             # cbar = plt.colorbar(contour, ax=ax, orientation='horizontal', pad=0.05, shrink=0.8)
-            # cbar.set_label('Geopotential Height (gpm)')
\ No newline at end of file
+            # cbar.set_label('Geopotential Height (gpm)')
+
Index: vortexclust/core/utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import warnings\nfrom typing import List, Union\n\nimport pandas as pd\n\n\ndef norm_series_df(df: Union[pd.Series, pd.DataFrame]) -> pd.DataFrame:\n    r\"\"\"\n    Normalizes a pandas Series or DataFrame by ensuring it's a DataFrame with reset index.\n\n    :param df: Input data to normalize\n    :type df: pd.Series or pd.DataFrame\n\n    :raises TypeError: If input is neither a pandas Series nor DataFrame\n\n    :return: A pandas DataFrame with reset index\n    :rtype: pd.DataFrame\n    \"\"\"\n    if isinstance(df, pd.Series):\n        df = df.to_frame().T\n    elif not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Expected pd.Series or pd.DataFrame\")\n    return df.reset_index(drop=True)\n\n\ndef validate_columns(df: pd.DataFrame, required_cols: List[str]) -> None:\n    r\"\"\"\n    Validates that a DataFrame contains all required columns.\n\n    :param df: The DataFrame to validate\n    :type df: pd.DataFrame\n    :param required_cols: List of column names that must be present in the DataFrame\n    :type required_cols: list\n\n    :raises KeyError: If any required columns are missing from the DataFrame\n    :raises TypeError: If input is neither a pandas Series nor DataFrame.\n    :raises UserWarning: If length of required_cols is below 1.\n\n    :return: None\n    \"\"\"\n    if (not isinstance(df, pd.DataFrame)) or (not isinstance(df, pd.Series)):\n        raise TypeError(\"Expected a pandas.DataFrame or pandas.Series\")\n    if len(required_cols) < 1:\n        warnings.warn(UserWarning(\"At least one required column should be given.\"))\n\n    missing = [col for col in required_cols if col not in df.columns]\n    if missing:\n        raise KeyError(f\"Missing required columns: {missing}\")\n\n\n# Build candidate event ranges\ndef get_event_ranges(flag_array, days=7):\n    ranges = []\n    i = 0\n    while i <= len(flag_array) - days:\n        if flag_array[i]:\n            j = i\n            while j < len(flag_array) and flag_array[j]:\n                j += 1\n            if j - i >= days:\n                ranges.append((i, j))  # (start_idx, end_idx)\n                i = j  # Skip ahead\n            else:\n                i = j\n        else:\n            i += 1\n    return ranges
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/vortexclust/core/utils.py b/vortexclust/core/utils.py
--- a/vortexclust/core/utils.py	(revision 66ec76aea3fd9febcbe4e5d401f791f9440a4b84)
+++ b/vortexclust/core/utils.py	(date 1755027440049)
@@ -50,6 +50,17 @@
 
 # Build candidate event ranges
 def get_event_ranges(flag_array, days=7):
+    r"""
+    Build candidate event ranges from a boolean flag array.
+
+    :param flag_array: Boolean array indicating event days.
+    :type flag_array: array-like
+    :param days: Minimum consecutive days to qualify as an event, defaults to 7.
+    :type days: int, optional
+
+    :return: List of tuples with start and end indices of detected events.
+    :rtype: list[tuple]
+    """
     ranges = []
     i = 0
     while i <= len(flag_array) - days:
Index: vortexclust/analysis/decomposition.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import warnings\nfrom typing import Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.lib.stride_tricks import sliding_window_view\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom vortexclust.io.paths import check_path\n\n\ndef compute_pca(df: pd.DataFrame, n_comp: int = 4, **kwargs) -> Tuple[np.ndarray, pd.DataFrame, PCA]:\n    r\"\"\"\n    Computes Principal Component Analysis (PCA) for a given dataset.\n\n    :param df: Input data containing numerical data.\n    :type df: pandas.DataFrame\n    :param n_comp: Number of principal components to retain.\n    :type n_comp: int\n    :param kwargs:\n        `scaler` (object or type, optional): A scaler instance or class. Default is StandardScaler. If no scaler should be used, `scaler`has to be set explicitly to `None`\n        `savecsv` (str, optional): File path to save PCA scores.\n\n    :raises ValueError: If `comp` exceeds the number of input features in `df`.\n    :raises TypeError: If `df` is not a Pandas DataFrame or contains non-numeric data.\n    :raises FileNotFoundError: If `savefig` or `savecsv` directories do not exist.\n\n    :return: Tuple of np.ndarray containing the transformed data,\n    a DataFrame containing principal component loadings and explained variance statistics and pca.\n    :rtype: Tuple[numpy.ndarray, pandas.DataFrame, PCA object]\n    \"\"\"\n    # plot_type = kwargs.get('plot_type', '2D')\n    scaler = kwargs.get('scaler', StandardScaler)\n    savecsv = kwargs.get('savecsv', None)\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input data must be a Pandas DataFrame.\")\n\n    if 'label' in df.columns:\n        data = df.drop('label', axis=1)\n    else:\n        data = df\n\n    try:\n        # Check if scaler is a class, not an instance\n        if callable(scaler):\n            scaler = scaler()\n\n            scaler.fit(data)\n            X = scaler.transform(data)\n        else:\n            X = data\n    except TypeError as e:\n        raise TypeError(f'Type Error: {e}. \\n Ensure your dataframe has only numeric types.')\n\n    # compute PCA\n    try:\n        pca = PCA()\n        pca.fit(X)\n        x_new = pca.transform(X)\n    except Exception as e:\n        raise Exception(f\"Error while transforming X to PCA: {e}\")\n\n    if n_comp > pca.components_.shape[1]:\n        warnings.warn(UserWarning(f'Less components than given. Reset n_comp to {pca.components_.shape[1]}'))\n        n_comp = pca.components_.shape[1]\n\n    # generate overview of influence of each features on each principal component\n    scores = pd.DataFrame(pca.components_[:n_comp].T,\n                          columns=[f'PC{i}' for i in range(n_comp)],\n                          index=data.columns)\n\n    expl_var_row = pd.DataFrame([pca.explained_variance_[:n_comp], pca.explained_variance_ratio_[:n_comp]],\n                                columns=[f\"PC{i}\" for i in range(n_comp)],\n                                index=['Expl_var', 'Expl_var_ratio'])\n    scores = pd.concat([scores, expl_var_row])\n\n    # store in csv\n    if savecsv:\n        check_path(savecsv)\n        scores.to_csv(savecsv)\n\n    return x_new, scores, pca\n\ndef compute_eeof(signal, M=400, n_components=9):\n    r\"\"\"\n    :param signal:\n    :param M:\n    :param n_components:\n\n    :raises ValueError: If `M` is not an integer or greater than the number of time series.\n\n    :return:\n    \"\"\"\n    n = len(signal)\n    if n < M:\n        raise ValueError(\"Time series length must be greater than window size M.\")\n\n    # Build the delay-embedded matrix\n    delay_matrix = sliding_window_view(signal, window_shape=M)\n\n    # Apply PCA to the delay matrix\n    pca = PCA(n_components=n_components)\n    epcs = pca.fit_transform(delay_matrix)\n    eeofs = pca.components_\n    expl_var_ratio = pca.explained_variance_ratio_\n\n    # Reconstruct the signal using the first n_components\n    reconstructed = pca.inverse_transform(epcs)\n    # reconstructed = delay_matrix_recon.mean(axis=1)\n\n    # Pad reconstructed to match original length\n    full_reconstructed = np.full((n,M), np.nan)\n    full_reconstructed[M - 1:] = reconstructed\n\n    return epcs, eeofs, expl_var_ratio, full_reconstructed, delay_matrix\n\n\ndef multivariate_autocorrelation(X, lag=1):\n    X = pd.DataFrame(X)\n    return [X[col].autocorr(lag=lag) for col in X.columns]
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/vortexclust/analysis/decomposition.py b/vortexclust/analysis/decomposition.py
--- a/vortexclust/analysis/decomposition.py	(revision 66ec76aea3fd9febcbe4e5d401f791f9440a4b84)
+++ b/vortexclust/analysis/decomposition.py	(date 1755027440048)
@@ -118,5 +118,16 @@
 
 
 def multivariate_autocorrelation(X, lag=1):
+    r"""
+    Computes the autocorrelation for each column in a multivariate time series.
+
+    :param X: Input data containing multiple time series.
+    :type X: array-like or pandas.DataFrame
+    :param lag: Lag at which to compute the autocorrelation, defaults to 1.
+    :type lag: int, optional
+
+    :return: List with the autocorrelation for each column.
+    :rtype: list[float]
+    """
     X = pd.DataFrame(X)
     return [X[col].autocorr(lag=lag) for col in X.columns]
\ No newline at end of file
Index: vortexclust/workflows/demo.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import warnings\nfrom vortexclust.io.paths import check_path\n\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\ndef plot_ssa_grid(data_series, ssa_results, labels,\n                  index_format=['%b %d'],\n                  figsize=(15, 8),\n                  used_signals = 1,\n                  titles = None):\n    \"\"\"\n    Plot a grid of original vs SSA seasonal component time series.\n\n    Parameters:\n    - data_series: list of original time series (e.g., [avg_year, avg_day_over_year])\n    - ssa_results: list of SSA seasonal components (same shape as data_series)\n    - labels: list of labels (same shape: [ [label1, label2, ...], [label1, label2, ...] ])\n    - index_format: datetime x-axis tick format, default is month-day\n    - figsize: overall figure size\n    - used_signals: number of SSA components to reconstruct signal\n    - titles: list of titles for each subplot\n    \"\"\"\n    m = len(data_series)\n    n = len(labels)\n\n    fig, ax = plt.subplots(m, n, figsize=figsize, squeeze=False)\n\n    for i in range(m):\n        if len(index_format) > 1:\n            format = index_format[i]\n        for j in range(n):\n            ax_ij = ax[i][j]\n\n            ts = data_series[i]\n            ssa = ssa_results[i][j]\n            label = labels[j]\n\n            if used_signals > ssa[:].shape[0]:\n                print(f\"Number of given signals too high. Reset to maximum number {ssa.shape[0]} of signals in SSA.\")\n                used_signals = ssa.shape[0]\n            if used_signals > 1:\n                reconstructed = ssa[:][:used_signals].sum(axis=0)\n\n            seasonal = ssa[:][0]\n\n            if titles:\n                title = str(label) + ' ' + titles[i]\n            else:\n                title = None\n\n            ax_ij.plot(ts.index, ts[label], label='Averaged data')\n            ax_ij.plot(ts.index, seasonal, label='Trend', linestyle='--')\n            if used_signals > 1: ax_ij.plot(ts.index, reconstructed, label='Reconstructed', linestyle='-.')\n\n            if title: ax_ij.set_title(title)\n            ax_ij.xaxis.set_major_formatter(mdates.DateFormatter(format))\n            ax_ij.tick_params(axis='x', rotation=45)\n            ax_ij.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_timeseries_moments(df, columns, labels,\n                            time_column='string', time_format='%m-%y',\n                            time_span=-1,\n                            positions=None, vertical_line=None,\n                            title=None, savefig=None,\n                            num_plots=2, figsize=(10, 5)):\n    if figsize:\n        fig, axes = plt.subplots(num_plots, figsize=figsize)\n    else:\n        fig, axes = plt.subplots(num_plots, figsize=(10, 2.5*num_plots))\n\n    if num_plots > 1:\n        axes[0].set_title(title)\n    else:\n        axes.set_title(title)\n\n    if len(columns) % 2 == 1:\n        warnings.warn(UserWarning(\n            'Give an even number of columns. Two columns will be plotted in one plot. Removing last column.'))\n        columns = columns[:-1]\n\n    for i in range(len(axes)):\n        if num_plots > 1:\n            ax = axes[i]\n        else:\n            ax = axes\n        ax.plot(df[columns[2*i]][:time_span], label=labels[2*i])\n        ax.plot(df[columns[2*i + 1]][:time_span], label=labels[2*i + 1])\n        if isinstance(vertical_line, int):\n            ax.axvline(x=vertical_line, color='black', linestyle='--')\n\n        ax.legend(loc='upper right')\n\n        # Add grey vertical lines at positions and the corresponding date\n        if positions is not None:\n            ax.set_xticks(positions)\n            ax_top = ax.twiny()\n            ax_top.set_xlim(ax.get_xlim())  # Align with bottom axis\n            ax_top.set_xticks(positions)\n\n            if time_column in df.columns:\n                labels_dt = df.iloc[positions][time_column].dt.strftime(time_format)\n                ax_top.set_xticklabels(labels_dt, ha='center', rotation=0)\n            else:\n                ax_top.set_xticklabels([str(x) for x in positions], ha='center', rotation=0)\n\n            ax_top.tick_params(axis='x', direction='in', pad=5)\n            ax_top.xaxis.set_label_position('top')\n\n    plt.tight_layout()\n    if savefig is not None:\n        check_path(savefig)\n        plt.savefig('../output/scaled_moments.png')\n    plt.show()\n\n\ndef plot_eeof(epc, eeof, expl_var_ratio, savefig=None):\n    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n    ax[0][0].scatter(np.arange(1, len(expl_var_ratio)+1), expl_var_ratio*100)\n    ax[0][0].set_ylabel('Eigenvalues (%)')\n    ax[0][0].set_xlabel('Rank')\n    ax[0][0].set_title('Eigenvalue Spectrum')\n\n    ax[1][1].plot(epc[:, 0], epc[:, 1])\n    ax[1][1].set_xlabel('EPC1')\n    ax[1][1].set_ylabel('EPC2')\n    ax[1][1].set_title('EPC1 vs. EPC2')\n\n    # ax[0].plot(era5_all['scaled_area'][:2000], label='Data')\n    ax[0][1].plot(eeof[0, :400], label='EEOF1')\n    ax[0][1].plot(eeof[1, :400], label='EEOF2')\n    ax[0][1].set_xlabel('Time lag (day)')\n    ax[0][1].set_ylabel('EEOF')\n    ax[0][1].set_title(\"EEOFs 1 and 2\")\n    ax[0][1].legend(loc='upper left')\n\n    ax[1][0].plot(epc[:2000, 0], label='EPC 1')\n    ax[1][0].plot(epc[:2000, 1], label='EPC 2')\n    ax[1][0].set_xlabel('Time (day)')\n    ax[1][0].set_ylabel('EPC')\n    ax[1][0].set_title(\"EPCs 1 and 2\")\n    ax[1][0].legend(loc = 'upper left')\n    plt.tight_layout()\n    if savefig:\n        check_path(savefig)\n        plt.savefig(savefig)\n    plt.show()\n\n\ndef plot_hist_per_class(df, feat_k, y, savefig=None):\n    # for idx, feat_k in enumerate(features_kopt):\n    var = y\n    for feature in feat_k['features']:\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n        unique_classes = sorted(df[var].dropna().unique())\n        palette = sns.color_palette('tab10', len(unique_classes))\n        color_map = dict(zip(unique_classes, palette))\n\n        # Histogram with percentages\n        sns.histplot(data=df,\n                     x=feature,\n                     hue=var,\n                     palette=color_map,\n                     hue_order=unique_classes,\n                     common_norm=False,\n                     multiple='dodge',\n                     stat='percent',\n                     kde=True,\n                     ax=axes[0])\n        axes[0].set_title(f\"Percentage Histogram of {feature} by {var}\")\n        axes[0].set_xlabel(feature)\n        axes[0].set_ylabel('Percent')\n\n        # Histogram with counts\n        sns.histplot(data=df,\n                     x=feature,\n                     hue=var,\n                     palette=color_map,\n                     hue_order=unique_classes,\n                     common_norm=False,\n                     multiple='dodge',\n                     stat='count',\n                     kde=False,\n                     ax=axes[1])\n        axes[1].set_title(f\"Count Histogram of {feature} by {var}\")\n        axes[1].set_xlabel(feature)\n        axes[1].set_ylabel('Count')\n\n        group_means = df.groupby(var)[feature].mean()\n        # Draw one vertical line at mean per class, with matching color\n        for i, label in enumerate(unique_classes):\n            mean_val = group_means[label]\n            color = color_map[label]\n            axes[0].axvline(mean_val, color=color, linewidth=1, linestyle='-.',\n                            label=f\"Mean of {var} = {label}\")\n            axes[1].axvline(mean_val, color=color, linewidth=1, linestyle='-.',\n                            label=f\"Mean of {var} = {label}\")\n        if savefig:\n            check_path(savefig)\n            plt.savefig(savefig)\n\n        plt.tight_layout()\n        plt.show()\n\n\ndef compare_cluster(df,\n                    compare_col,\n                    y_names=['y_ar_latcent', 'y_ar_latcent_scArea', 'y_ar_latcent_filteredArea', 'y_ar_latcent_u'],\n                    value_col='string',\n                    pred_value='S',\n                    gt_value='S',\n                    verbose=True):\n    r\"\"\"\n    Compare a cluster classification column with multiple threshold-based ground truth columns.\n\n    :param df: Pandas DataFrame with entire data\n    :type df: Pandas DataFrame\n    :param compare_col: String or integer of the column that should be compared.\n    :type compare_col: str\n    :param y_names: List of Strings with all column names that should be compared to ´compare_col´\n    :type y_names: List[String]\n    :param value_col: String or integer of column that should be counted\n    :type value_col: str\n    :param pred_value: String or integer of predicted value\n    :type pred_value: str or integer\n    :param gt_value: String or integer of ground truth value\n    :type gt_value: str or integer\n    :param verbose: Boolean whether to print information about accuracy, recall, precision and f1 score.\n    :type verbose: bool\n\n    :return: None\n    \"\"\"\n\n    if not compare_col in df.columns:\n        raise KeyError(f\"Compare column {compare_col} not found in dataframe columns.\")\n    if not value_col in df.columns:\n        raise KeyError(f\"Value column {value_col} not found in dataframe columns.\")\n\n    for y in y_names:\n        if y not in df.columns:\n            raise KeyError(f\"Prediction column {y} not found in dataframe.\")\n\n        y_vals = df[y].dropna().unique()\n        gt_vals = df[compare_col].dropna().unique()\n\n        if not pred_value in y_vals:\n            raise ValueError(f\"{pred_value} is not a found value in the given class column {y}.\"\n                             f\"Found {y_vals} instead.\")\n        if not gt_value in gt_vals:\n            raise ValueError(f\"{gt_value} is not a found value in the given class column {y}.\"\n                             f\"Found {gt_vals} instead.\")\n        print(f'----------------------------------------------')\n        print(f'Comparison of Percentages:  {y} (predicted: {pred_value}) vs {compare_col} (ground truth: {gt_value})')\n        table = pd.pivot_table(\n            data=df[[y, value_col, compare_col]],\n            values=value_col,\n            index=compare_col,\n            columns=y,\n            aggfunc='count', margins=True).fillna(0)\n        print(np.round(table / df.shape[0] * 100, 2))\n\n        tp = ((df[y] == pred_value) & (df[compare_col] == gt_value)).sum()\n        fp = ((df[y] == pred_value) & (df[compare_col] != gt_value)).sum()\n        fn = ((df[y] != pred_value) & (df[compare_col] == gt_value)).sum()\n        tn = ((df[y] != pred_value) & (df[compare_col] != gt_value)).sum()\n\n        acc = (tn + tp) / (tp + fp + tn + fn)\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = 2 * precision * recall / (precision + recall)\n\n        if verbose:\n            print(f\"Accuracy: {np.round(acc, 2)}\\n\"\n                  f\"Precision: {np.round(precision, 2)}\\n\"\n                  f\"Recall: {np.round(recall, 2)}\\n\"\n                  f\"F1 Score: {np.round(f1, 2)}\")\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/vortexclust/workflows/demo.py b/vortexclust/workflows/demo.py
--- a/vortexclust/workflows/demo.py	(revision 66ec76aea3fd9febcbe4e5d401f791f9440a4b84)
+++ b/vortexclust/workflows/demo.py	(date 1755027440049)
@@ -71,6 +71,36 @@
                             positions=None, vertical_line=None,
                             title=None, savefig=None,
                             num_plots=2, figsize=(10, 5)):
+    r"""
+    Plot multiple time series in paired subplots with optional annotations.
+
+    :param df: DataFrame containing the time series data.
+    :type df: pandas.DataFrame
+    :param columns: List of column names to plot. Must contain an even number of entries.
+    :type columns: list
+    :param labels: Labels for each column.
+    :type labels: list
+    :param time_column: Name of the column containing datetime values, defaults to 'string'.
+    :type time_column: str, optional
+    :param time_format: Format string for displaying dates, defaults to '%m-%y'.
+    :type time_format: str, optional
+    :param time_span: Number of rows to include from the start; -1 uses all rows, defaults to -1.
+    :type time_span: int, optional
+    :param positions: Positions at which vertical markers and top labels are drawn, defaults to None.
+    :type positions: list, optional
+    :param vertical_line: Position of a single vertical line, defaults to None.
+    :type vertical_line: int, optional
+    :param title: Title for the figure, defaults to None.
+    :type title: str, optional
+    :param savefig: Path to save the figure, defaults to None.
+    :type savefig: str, optional
+    :param num_plots: Number of subplots to create, defaults to 2.
+    :type num_plots: int, optional
+    :param figsize: Figure size, defaults to (10, 5).
+    :type figsize: tuple, optional
+
+    :return: None
+    """
     if figsize:
         fig, axes = plt.subplots(num_plots, figsize=figsize)
     else:
@@ -122,6 +152,20 @@
 
 
 def plot_eeof(epc, eeof, expl_var_ratio, savefig=None):
+    r"""
+    Visualize extended empirical orthogonal functions and related statistics.
+
+    :param epc: Extended principal components.
+    :type epc: numpy.ndarray
+    :param eeof: Extended empirical orthogonal functions.
+    :type eeof: numpy.ndarray
+    :param expl_var_ratio: Explained variance ratios for the components.
+    :type expl_var_ratio: array-like
+    :param savefig: Path to save the figure, defaults to None.
+    :type savefig: str, optional
+
+    :return: None
+    """
     fig, ax = plt.subplots(2, 2, figsize=(10, 8))
     ax[0][0].scatter(np.arange(1, len(expl_var_ratio)+1), expl_var_ratio*100)
     ax[0][0].set_ylabel('Eigenvalues (%)')
@@ -155,6 +199,20 @@
 
 
 def plot_hist_per_class(df, feat_k, y, savefig=None):
+    r"""
+    Plot feature histograms grouped by class labels.
+
+    :param df: DataFrame containing the data and class labels.
+    :type df: pandas.DataFrame
+    :param feat_k: Dictionary-like object with a list of feature names under ``'features'``.
+    :type feat_k: dict
+    :param y: Column name of the class variable.
+    :type y: str
+    :param savefig: Path to save the figure, defaults to None.
+    :type savefig: str, optional
+
+    :return: None
+    """
     # for idx, feat_k in enumerate(features_kopt):
     var = y
     for feature in feat_k['features']:
